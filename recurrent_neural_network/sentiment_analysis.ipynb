{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Sentiment Analysis of French Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDsXJRI3Wfrv"
   },
   "source": [
    "### Objectives\n",
    "1. Text cleaning\n",
    "2. Text preprocessing for custom embedding Neural Network\n",
    "3. Train RNN model for sentiment analysis\n",
    "\n",
    "‚ö†Ô∏è This notebook will be your final deliverable. \n",
    "- Make sure it can run \"restart and run all\"\n",
    "- Delete useless code cells\n",
    "- Do not \"clear output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvwURl10Wmw1"
   },
   "source": [
    "# 0. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 30,000 french reviews of movies, along with the binary class 1 (positive) or 0 (negative) score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 8472,
     "status": "ok",
     "timestamp": 1615382505157,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "IufC0UUhxyGC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>√áa commence √† devenir √©nervant d'avoir l'impre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J'ai aim√© ce film, si il ressemble a un docume...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Une grosse merde ce haneke ce faire produire p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beau m√©lodrame magnifiquement photographi√©, \"V...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A la poursuite du diamant vers est un film pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29946</th>\n",
       "      <td>Le meilleur film de super-h√©ros derri√®re le ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29947</th>\n",
       "      <td>Un drame qui est d'une efficacit√© remarquable....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29948</th>\n",
       "      <td>Une daube hollywoodienne de plus, aucun int√©r√™...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29949</th>\n",
       "      <td>Et voil√† un nouveau biopic sur la star du X Li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29950</th>\n",
       "      <td>Un film qui fait vieux, avec des acteurs pas t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29951 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  polarity\n",
       "0      √áa commence √† devenir √©nervant d'avoir l'impre...         0\n",
       "1      J'ai aim√© ce film, si il ressemble a un docume...         1\n",
       "2      Une grosse merde ce haneke ce faire produire p...         0\n",
       "3      Beau m√©lodrame magnifiquement photographi√©, \"V...         1\n",
       "4      A la poursuite du diamant vers est un film pro...         1\n",
       "...                                                  ...       ...\n",
       "29946  Le meilleur film de super-h√©ros derri√®re le ba...         1\n",
       "29947  Un drame qui est d'une efficacit√© remarquable....         1\n",
       "29948  Une daube hollywoodienne de plus, aucun int√©r√™...         0\n",
       "29949  Et voil√† un nouveau biopic sur la star du X Li...         0\n",
       "29950  Un film qui fait vieux, avec des acteurs pas t...         0\n",
       "\n",
       "[29951 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the dataset for you\n",
    "data = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/movies.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 8472,
     "status": "ok",
     "timestamp": 1615382505157,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "IufC0UUhxyGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    15051\n",
      "0    14900\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We create features\n",
    "y = data.polarity\n",
    "X = data.review\n",
    "\n",
    "# We analyse class balance\n",
    "print(pd.value_counts(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1615383356787,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "yzIpNmSg0XV4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity: 0 \n",
      "\n",
      "√áa commence √† devenir √©nervant d'avoir l'impression de voir et revoir le m√™me genre de film √† savoir : la com√©die romantique, surement le genre le plus prolifique de le production fran√ßaise actuelle. Le probl√®me c'est que l'on a souvent affaire √† des niaiseries de faible niveau comme celui ci. Avec un scenario ultra balis√© et conventionnel, c'est √† se demander comment √ßa peut passer les portes d'un producteur. Bref cette sempiternel histoire d'un homme mentant au nom de l'amour pour reconqu√©rir une femme et qui √† la fin se prend son mensonge en pleine figure est d'une originalit√© affligeante, et ce n'est pas la pr√©sence au casting de l'ex miss m√©t√©o Charlotte Le Bon qui r√™ve surement d'avoir la m√™me carri√®re que Louise Bourgoin qui change la donne.\n"
     ]
    }
   ],
   "source": [
    "# We check various reviews\n",
    "print(f'polarity: {y[0]} \\n')\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì We need to give a _quick & dirty_ cleaning to all the sentences in the dataset. Create a variable `X_clean` of similar shape, but with the following cleaning:\n",
    "- Replace french accents by their non-accentuated equivalent using the [unidecode.unidecode()](https://pypi.org/project/Unidecode/) method\n",
    "- Reduce all uppercases to lowercases\n",
    "- Remove any characters outside of a-z, for instance using `string.isalpha()`\n",
    "\n",
    "üòå You will be given the solution `X_clean` in the next question to make sure you can complete the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE\n",
    "X_clean=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /home/mikeplanche/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (1.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install unidecode\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing accents:\n",
    "X_clean=X_clean.apply(lambda x: unidecode(x))\n",
    "\n",
    "#lowering text\n",
    "X_clean=X_clean.apply(lambda x: x.lower())\n",
    "\n",
    "#removing punctuation\n",
    "import string \n",
    "\n",
    "def punctuation_removal(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "X_clean=X_clean.apply(lambda x: punctuation_removal(x))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest=X_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('C14',\n",
    "    shape = X_clean.shape,\n",
    "    first_sentence = X_clean[0]\n",
    ")\n",
    "result.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have clean sentences, we need to convert each one into a list of integers of fixed size\n",
    "- For example, the sentence: `\"this was good\"` should become something like `array([1, 3, 18, 0, 0, 0, ...0], dtype=int32)` where each integer match to a each _unique_ word in your corpus of sentences.\n",
    "\n",
    "‚ùì Create a numpy ndarray `X_input` of shape (29951, 100) that will be the direct input to your Neutral Network. \n",
    "\n",
    "- 29951 represents the number of reviews in the dataset `X_clean`\n",
    "- 100 represents the maximum number of words to keep for each movie review.\n",
    "- It must contain only numerical values, without any `NaN`\n",
    "- In the process, compute and save the number of _unique_ words in your cleaned corpus under `vocab_size` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ First, you **must** start back from the clean solution below (14Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29951,)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/movies_X_clean.csv\")['review']\n",
    "X_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE\n",
    "\n",
    "#splitting sentences into words\n",
    "def convert_sentences(X):\n",
    "    return [sentence.split(' ') for sentence in X]\n",
    "\n",
    "X_clean = convert_sentences(X_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a word to id dictionnary\n",
    "word_to_id = {}\n",
    "iter_ = 1\n",
    "for sentence in X_clean:\n",
    "    for word in sentence:\n",
    "        if word in word_to_id:\n",
    "            continue\n",
    "        word_to_id[word] = iter_\n",
    "        iter_ += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 62379 different words in the X_clean sentences\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(word_to_id)} different words in the X_clean sentences')\n",
    "vocab_size=len(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a tokenize function to swap words by their number_id\n",
    "def tokenize(sentences, word_to_id):\n",
    "    return [[word_to_id[_] for _ in s if _ in word_to_id] for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "X_clean = tokenize(X_clean, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#padding reviews in case they are les than 100 words long with 0 at the end\n",
    "X_clean = pad_sequences(X_clean, dtype='float32', padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59., 102., 103.,  72., 104.,  72., 105., 106., 100.,  28., 107.,\n",
       "         8., 108., 109., 110., 111., 112., 113.,  25., 114., 115.,  25.,\n",
       "        46., 116.,  72.,  25.,  94., 117., 113.,  25.,  46., 118., 119.,\n",
       "       120., 121., 122.,  64., 123.,  69., 124.,  19., 125.,  25., 126.,\n",
       "       127.,   6., 103.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padding worked\n",
    "X_clean[2][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29951, 352)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to keep 100 first words only for each review\n",
    "X_clean_final=[]\n",
    "for i in range(len(X_clean)):\n",
    "    X_clean_final.append(X_clean[i][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     2,     3, ...,     6,    76,    77],\n",
       "       [   86,    72,    13, ...,     0,     0,     0],\n",
       "       [   59,   102,   103, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [   59,  2439,  5003, ...,     0,     0,     0],\n",
       "       [    8,  1208,    36, ..., 41774, 62377,    10],\n",
       "       [   36,    13,    61, ...,  3039,    25,    10]], dtype=int32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input=np.reshape(X_clean_final,(29951,100))\n",
    "X_input=X_input.astype('int32')\n",
    "X_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('C1415',\n",
    "    type_X = type(X_input),\n",
    "    shape = X_input.shape, \n",
    "    input_1 = X_input[1], \n",
    ")\n",
    "result.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjM5UP5ZMbY_"
   },
   "source": [
    "# 3. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCreate and fit a Neural Netork that takes `X_input` and `y` as input, to binary classify each sentence's sentiment\n",
    "\n",
    "- You cannot use transfer learning or other pre-existing Word2Vec models\n",
    "- You must use a \"recurrent\" architecture to _capture_ a notion of order in the sentences' words\n",
    "- The performance metrics for this task is \"accuracy\"\n",
    "- Store your model in a variable `model` \n",
    "- Store the result your `model.fit()` in a variable `history`. \n",
    "- ‚ö†Ô∏è `history.history` must comprises a measure of the `val_accuracy` at each epoch.\n",
    "- You don't need to cross-validate your model\n",
    "\n",
    "üòå Don't worry, you will not be judged on your computer power: You should be able to reach accuracy significantly better than baseline in less than 3 minutes even without GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ But first, you **must** start back from the solution below (70Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/movies_X_input.csv'\n",
    "X_input = np.genfromtxt(url, delimiter=',', dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_input, y, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers \n",
    "\n",
    "def init_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size+1, output_dim=30, mask_zero=True))\n",
    "    model.add(layers.LSTM(10))\n",
    "    model.add(layers.Dense(5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "459/459 [==============================] - 82s 174ms/step - loss: 0.5594 - accuracy: 0.7049 - val_loss: 0.3121 - val_accuracy: 0.8760\n",
      "Epoch 2/10\n",
      "459/459 [==============================] - 81s 177ms/step - loss: 0.2311 - accuracy: 0.9114 - val_loss: 0.2861 - val_accuracy: 0.8852\n",
      "Epoch 3/10\n",
      "459/459 [==============================] - 78s 169ms/step - loss: 0.1527 - accuracy: 0.9431 - val_loss: 0.2978 - val_accuracy: 0.8819\n",
      "Epoch 4/10\n",
      "459/459 [==============================] - 86s 186ms/step - loss: 0.1251 - accuracy: 0.9548 - val_loss: 0.3289 - val_accuracy: 0.8785\n",
      "Epoch 5/10\n",
      "459/459 [==============================] - 79s 172ms/step - loss: 0.1043 - accuracy: 0.9613 - val_loss: 0.3718 - val_accuracy: 0.8773\n",
      "Epoch 6/10\n",
      "459/459 [==============================] - 72s 156ms/step - loss: 0.0890 - accuracy: 0.9690 - val_loss: 0.4045 - val_accuracy: 0.8744\n",
      "Epoch 7/10\n",
      "459/459 [==============================] - 74s 161ms/step - loss: 0.0778 - accuracy: 0.9716 - val_loss: 0.4096 - val_accuracy: 0.8739\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = init_model(len(word_to_id))\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "history=model.fit(X_train, y_train, \n",
    "          epochs=10, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.4468790888786316,\n",
       "  0.22659480571746826,\n",
       "  0.16633781790733337,\n",
       "  0.13486681878566742,\n",
       "  0.11261698603630066,\n",
       "  0.09949854761362076,\n",
       "  0.0876273363828659],\n",
       " 'accuracy': [0.7979556918144226,\n",
       "  0.9123679995536804,\n",
       "  0.9363543391227722,\n",
       "  0.9496422410011292,\n",
       "  0.9582964181900024,\n",
       "  0.9644293189048767,\n",
       "  0.9676320552825928],\n",
       " 'val_loss': [0.3121359348297119,\n",
       "  0.28613102436065674,\n",
       "  0.29783540964126587,\n",
       "  0.3289003372192383,\n",
       "  0.37181761860847473,\n",
       "  0.4045064449310303,\n",
       "  0.4096420705318451],\n",
       " 'val_accuracy': [0.8759936690330505,\n",
       "  0.885214626789093,\n",
       "  0.8818759918212891,\n",
       "  0.8785373568534851,\n",
       "  0.8772655129432678,\n",
       "  0.8744038343429565,\n",
       "  0.8739268779754639]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('C1517',\n",
    "                         history=history.history)\n",
    "result.write()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNt966tqZXM2p288pQsUAUV",
   "name": "certification_DL_NLP",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
